<<<<<<< HEAD
knit("PA1_template.Rmd")
install.packages(c("car", "dendextend", "googleVis", "NLP", "RCurl", "rjson", "RJSONIO", "xlsx"))
require('dev')
#Only the first time data is download, a "data" directory is created.
if (!file.exists("data")) {
dir.create("data")
}
#and stores that file into "data" directory
if(!file.exists("data/repdata-data-StormData.csv.bz2")) {
fileURL <- "http://d396qusza40orc.cloudfront.net/repdata%2Fdata%2FStormData.csv.bz2"
download.file(fileURL, destfile = "./data/repdata-data-StormData.csv.bz2")
}
#Uncompress and reads if not exists. Stores data in storm.noaa.data variable.
if(!file.exists("data/storm_noaa_data.dat")){
storm.noaa.data <- read.csv(bzfile("./data/repdata-data-StormData.csv.bz2"))
write.table(storm.noaa.data, "data/storm_noaa_data.dat")
}else{
storm.noaa.data <- read.table("data/storm_noaa_data.dat")
}
#subsets those requested columns and stores in work.data variable.
work.data <- subset(storm.noaa.data, FATALITIES > 0 | INJURIES > 0 | PROPDMG > 0 | CROPDMG > 0, select = c(8, 23:28))
#transforms exponents into numeric values hecto=100, kilo=1000, million=1000000=1e+06 etc..
expcalc <- function(dam,exp) {
if (exp == "") {
dam * 1
} else if (exp == "1") {
dam * 10
} else if (exp == "H" | exp == "h") {
dam * 100
} else if (exp == "K" | exp == "k") {
dam * 1000
} else if (exp == "M" | exp == "m") {
dam * 1e+06
} else if (exp == "B" | exp == "b") {
dam * 1e+09
} else 0
}
#creates two new columns,applying those exponents to property damage(PDCOST), and crop damage(CROPCOST)
work.data$PDCOST <- mapply(expcalc, work.data$PROPDMG, work.data$PROPDMGEXP)
work.data$CROPCOST <- mapply(expcalc, work.data$CROPDMG, work.data$CROPDMGEXP)
##trying to get tidier data, transform EVTYPE column to lower case. Makes it as factor again.
work.data$EVTYPE <- tolower(work.data$EVTYPE)
work.data$EVTYPE <- as.factor(work.data$EVTYPE)
require(ggplot2)
require(reshape2)
# melt function needs reshape2 package
#melt function: id.vars is a vector of id variables. measure.vars is a vector of measured variables
#In pop.health stores 3 columns: "EVTYPE"   "variable" "value"
pop.health <- melt(work.data[,c(1:3)], id.vars = "EVTYPE", measure.vars= c("FATALITIES", "INJURIES"))
#In pop.health.agg stores 3 columns: "EVTYPE"        "Casualty" "Count"
#there are 488 events, but 2 types: fatalities and injuries, so 976 rows
pop.health.agg <- setNames(aggregate(value ~ EVTYPE+variable, data=pop.health, sum, na.rm = TRUE), c("EVTYPE","Casualty","Count"))
#In total stores each 447 type of events and the sum/Count each one
total <- aggregate(Count ~ EVTYPE, pop.health.agg, sum)
#In pop.health.agg2 stores "EVTYPE" "Casualty" "Count.x" "Count.y"
pop.health.agg2 <- merge(pop.health.agg, total, by = "EVTYPE")
#orders and select first 20
pop.health.agg2 <- pop.health.agg2[order(-pop.health.agg2$Count.y, pop.health.agg2$Casualty), ][1:20, ]
#Prepares by factoring to plot
pop.health.agg2$EVTYPE <-suppressWarnings(factor(pop.health.agg2$EVTYPE, levels=pop.health.agg2[order(pop.health.agg2$Count.y), "EVTYPE"]))
ggplot(pop.health.agg2, aes(x = EVTYPE, y = Count.x, fill = Casualty)) + geom_bar(stat = "identity") + coord_flip() +
ylab("Fatality Injury") +
xlab("Type of Event") +
ggtitle("Fatalities and Injuries by Type of Event")
storm <- read.csv("storm_data.csv.bz2")
str(storm)
library(AppliedPredictiveModeling)
library(caret)
data(AlzheimerDisease)
install.packages('caret')
library(AppliedPredictiveModeling)
library(caret)
data(AlzheimerDisease)
install.packages('AppliedPredictiveModeling')
library(AppliedPredictiveModeling)
library(caret)
data(AlzheimerDisease)
require('shiny')
install.packages(c("bdsmatrix", "BH", "ChainLadder", "DBI", "FactoMineR", "formatR", "gsubfn", "httr", "labeling", "markdown", "NLP", "packrat", "RCurl", "rjson", "RJSONIO", "sandwich", "spacetime", "swirl", "xlsx", "xlsxjars"))
install.packages(c("bdsmatrix", "ChainLadder", "DBI", "FactoMineR", "formatR", "gsubfn", "httr", "labeling", "markdown", "NLP", "packrat", "RCurl", "rjson", "RJSONIO", "sandwich", "spacetime", "swirl", "xlsx", "xlsxjars"))
install.packages(c("bdsmatrix", "RCurl", "rjson", "RJSONIO", "xlsx", "xlsxjars"))
install.packages('bdsmatrix')
install.packages("bdsmatrix")
install.packages("bdsmatrix")
install.packages("bdsmatrix")
install.packages(c("bdsmatrix", "RCurl", "rjson", "RJSONIO", "xlsx", "xlsxjars"))
require('shiny')
require('kntri')
require('knitr')
getwd()
mydir <- paste0('getwd(),C:/Users/Scibrokes Trading/Documents/GitHub/englianhu/RepData_PeerAssessment2')
setwd <- mydir
library(knitr)
library(markdown)
# knitr configuration
opts_knit$set(progress=FALSE)
opts_chunk$set(echo=TRUE, message=FALSE, tidy=TRUE, comment=NA,
fig.path="figure/", fig.keep="high", fig.width=10, fig.height=6,
fig.align="center")
knit2html('stormDataAnalysis.Rmd')
browseURL('stormDataAnalysis.html')
mydir <- paste0('getwd(),C:/Users/Scibrokes Trading/Documents/GitHub/englianhu/RepData_PeerAssessment1')
setwd <- mydir
library(knitr)
library(markdown)
# knitr configuration
opts_knit$set(progress=FALSE)
opts_chunk$set(echo=TRUE, message=FALSE, tidy=TRUE, comment=NA,
fig.path="figure/", fig.keep="high", fig.width=10, fig.height=6,
fig.align="center")
knit2html('PA1_template.Rmd')
browseURL('PA1_template.html')
mydir
mydir <- paste0(getwd(),'C:/Users/Scibrokes Trading/Documents/GitHub/englianhu/RepData_PeerAssessment1')
setwd <- mydir
library(knitr)
library(markdown)
# knitr configuration
opts_knit$set(progress=FALSE)
opts_chunk$set(echo=TRUE, message=FALSE, tidy=TRUE, comment=NA,
fig.path="figure/", fig.keep="high", fig.width=10, fig.height=6,
fig.align="center")
knit2html('PA1_template.Rmd')
browseURL('PA1_template.html')
mydir
install.packages("rpubchem")
require('rpubchem')
require(markdown)
library(knitr)
library(markdown)
library(Hmisc)
library(reshape)
library(ggplot2)
library(car)
library(knitr)
library(markdown)
library(formatR)
library(Hmisc)
library(reshape)
library(ggplot2)
library(car)
# knitr configuration
opts_knit$set(progress=FALSE)
opts_chunk$set(echo=TRUE, message=FALSE, tidy=TRUE, comment=NA,
fig.path="figure/", fig.keep="high", fig.width=10, fig.height=6,
fig.align="center")
rpubsUpload(title, htmlFile, id = NULL, properties = list(),
method = getOption("rpubs.upload.method", "internal"))
knit2html('stormDataAnalysis.Rmd')
browseURL('stormDataAnalysis.html')
rpubsUpload(title, htmlFile, id = NULL, properties = list(),
method = getOption("rpubs.upload.method", "internal"))
ToothGrowth
require('swirl')
swirl()
install_from_swirl('Regression Models')
swirl()
plot(child~parent, galton)
plot(jitter(child,4)~parent,galton)
regline <- lm(child~parent,galton)
regrline <- lm(child~parent, galton)
abline(regrline, lwd=3, col='red')
summary(regrline)
lm(child~parent,galton)
fit <- lm(child~parent,galton)
summary(fit)
fit$residuals
mean(fit$residuals)
cov(fit$residuals, galton$parent)
fit$coef[1]
ols.ic <- fit$coef[1]
ols.slope <- fit$coef[2]
lhs-rhs
lhs==rhs
all.equal(lhs,rhs)
ols(lhs~rhs)
ls(lhs~rhs)
as.environment(pos)
var(lhs)
varChile <- var(galton$child)
varChild <- var(galton$child)
fit$residuals
varRes <- var(fit$residuals)
est(ols.slope~ols.ic)
varEst <- est(ols.slope~ols.ic)
varEst <- est(y~ols.slope+ols.ic)
varEst <- est(ols.slope~ols.ic+1)
varEst <- est(ols.slope~ols.ic+0)
args(est)
varEst <- var(est(ols.slope, ols.ic))
all.equal(varChild, sum(varRes,varEst))
all.equal(varChild, varRes+varEst)
efit <- lm(accel~mag+dist,attenu)
mean(efit$residuals)
cov(efit)
cov(predict(efit))
efit
cov(efit$residuals,attenu$mag)
cov(efit$residuals,attenu$dist)
cor(efits)
efits
args(cor)
cor(gpa_nor,gch_nor)
args(lm)
l_nor <- lm(gch_nor ~ gpa_nor)
args(lm)
fit <- lm(child~parent, galton)
sqrt(n-2)
sqrt(sum(fit$residuals^2)/(n-2))
summary(fit)$sigma
sqrt(deviance(fit)/(n-2))
args(expr)
args(expression)
mu <- mean(galton$child)
args(sqrt)
sTot <- sum((galton$child-mu)^2)
args(res)
args(residuals)
sRes <- deviance(fit)
sRes/sTot
1-sRes/sTot
summary(fit)$r
summary(fit)$r.squared
args(core)
args(cor)
cor(galton$parent, galton$child)^2
ones <- rep(1, nrow(galton))
lm(child ~ ones + parent - 1, galton)
lm(child ~ parent, galton)
lm(child~1, galton)
head(trees)
fit <- lm(Volume ~ Girth + Height + Constant-1, trees)
trees2 <- eliminate("Girth", trees)
head(trees2)
fit2 <- lm(Volume ~ Height + Constant -1, trees2)
lapply(list(fit, fit2), coef)
args(lm)
all <- lm(Fertility~., swiss)
summary(all)
summary(all)
summary(lm(Fertility~Agriculture, swiss))
args(cor)
cor(swiss$Examination, swiss$Education)
swiss[1:2,]
cor(swiss$Agriculture, swiss$Education)
makelms()
swiss$Catholic
ec <- swiss$Examination+swiss$Catholic
args(efit)
efit <- lm(Fertility~. + ex, swiss)
efit <- lm(Fertility~. + ec, swiss)
efit$coef
all$coefficients-efit$coefficients
Multivar_Examples2
dataset(Multivar_Examples2)
datasets(Multivar_Examples2)
args(Insect)
InsectSprays
nrow(InsectSprays)
swirl()
nrow(InsectSprays)
names(InsectSprays)
levels(InsectSprays$spray)
length(levels(InsectSprays$spray))
str(InsectSprays)
string(InsectSprays)
dim(InsectSprays)
InsectSprays[15,]
head(InsectSprays,15)
arr
array
SB
InsectSprays
args(array)
info()
S8
SB
sB
M[,2]
args(array)
summary(InsectSprays[,2])
args(sapply)
sapply(InsectSprays,class)
args(lm)
fit <- lm(count~spray, InsectSprays)
x$coef
fit$coef
summary(fit)$coef
args(array)
est <- summary(fit)$coef[,1]
args(groups)
args(dummy)
InsectSprays
mean(SA)
mean(sA)
mean(sprayB)
sprayB
args(mean)
mean(sB)
args(lm)
nfit <- lm(count~spray-1,InsectSprays)
summary(nfit)
summary(nfit)$coef
InsectSprays$spray
spray2 <- relevel(InsectSprays$spray,\"C\")
spray2 <- relevel(InsectSprays$spray,'C')
fit2 <- lm(InsectSprays)
fit2 <- lm(count~spray2,InsectSprays)
args(fit2)
summary(fit2)$coef
args(array)
mean(sC)
args(fit$coef[2])
(fit$coef[2]-fit$coef[3])/1.6011
dim(hunger)
names(hunger)
dim(hunger)
swirl()
nrow(hunger)
names(hunger)
args(lm)
fit <- lm(hunger$Numeric ~ hunger$Year)
summary(fit)
summary(fit)$coef
args(lmF)
[hunger$Sex=="Female"]
args(lm)
lmF <- lm(Numeric[Sex=="Female"] ~ Year[Sex=="Female"],hunger)
lmM
args(lm)
lmM <- lm(Numeric[Sex=="Male"] ~ Year[Sex=="Male"],hunger)
args(lmBoth)
args(lm)
lmBoth <- lm(hunger$Numeric ~ hunger$Year + hunger$Sex)
args(lm)
summary(lmBoth)
args(lm)
lmInter <- lm(hunger$Numeric ~ hunger$Year + hunger$Sex + hunger$Year * hunger$Sex)
summary(lmInter)
fit <- lm(y ~ x, out2)
lm$residuals
summary(lm)$residuals
summary(fit)
summary(fit)$residuals
plot(fit, which=1)
out2[-1, ]
fitno <- lm(y ~ x, out2[-1, ])
plot(residuals~fitted, fitno)
plot(lm(residuals~fitted, fitno))
plot(lm(fitno$residuals~fitno$fitted, fitno))
args(lm)
plot(fitno, which=1)
coef(fit)
coef(fit)-coef(fitno)
dfbeta(fit)
head(dfbeta(fit))
resno <- out2[1, "y"] - predict(fitno, out2[1,])
1-resid(fit)[1]/resno
head(hatvalues(fit))
args(lm)
sigma <- sqrt(deviance(fit)/df.residual(fit))
sigma*sqrt(1-hatvalues(fit))
rstd <- resid(fit)/(sigma * sqrt(1-hatvalues(fit)))
head(cbind(rstd, rstandard(fit)))
plot(fit, which=3)
plot(fit, which=2)
args(lm)
sigma1 <- sqrt(deviance(fitno)/df.residual(fitno))
sqrt(1-hatvalues(fit)[1])
resid(fit)[1]/(sigma1*sqrt(1-hatvalues(fit)[1]))
head(rstudent(fit))
predict(fit, out2)
dy <- predict(fitno, out2)-predict(fit, out2)
2*sigma^2
sum(dy^2)/(2*sigma^2)
plot(fit, which=5)
args(lm)
rpg1()
rgp1()
rgp2()
head(swiss)
args(lm)
mdl <- lm(Fertility ~ ., swiss)
vif(mdl)
mdl2
args(lm)
mdl2 <- lm(Fertility ~ . -Examination, swiss)
vif(mdl2)
args(lm)
x1c <- simbias()
apply(x1c, 1, mean)
args(lm)
fit1 <- lm(Fertility ~ Agriculture, swiss)
args(lm)
fit3 <- lm(Fertility ~ Agriculture + Examination + Education, swiss)
args(lm)
anova(fit1, fit3)
args(lm)
deviance(fit3)
args(lm)
d <- deviance(fit3)/43
args(lm)
n <- (deviance(fit1) - deviance(fit3))/2
args(lm)
n/d
args(lm)
pf(n/d, 2, 43, lower.tail=FALSE)
args(lm)
shapiro.test(fit3$residuals)
args(lm)
anova(fit1, fit3, fit5, fit6)
args(lm)
View(ravenData)
args(lm)
mdl <- glm(ravenWinNum ~ ravenScore, binomial, ravenData)
args(lm)
lodds <- predict(mdl, data.frame(ravenScore=c(0, 3, 6)))
exp(lodds)/(1+exp(lodds))
summary(mdl)
args(lm)
exp(confint(mdl))
anova(mdl)
args(lm)
qchisq(0.95, 1)
args(lm)
var(rpois(1000, 50))
head(hits)
class(hits[,'date'])
as.integer(head(hits[,'date']))
mdl <- glm(visits ~ date, poisson, hits)
summary(mdl)
exp(b1)
confint(mdl, 'date')
exp(confint(mdl, 'date'))
which.max(hits[,'visits']
)
hits[704,]
mdl$fitted.values[704]
lambda <- mdl$fitted.values[704]
qpois(.95, lambda)
args(glm)
mdl2 <- glm(formula = simplystats ~ date, family = poisson, data = hits, offset = log(visits + 1))
qpois(.95, mdl2$fitted.values[704])
require('swirl')
require('caret')
library(kernlab)
install.packages(kernlab)
## Read the dataset
train <- read.csv('https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv', header=T, sep=',', na.strings='?')
=======
lines(Sub_metering_2~Datetime,col='Red')
lines(Sub_metering_3~Datetime,col='Blue')
})
legend("topright", col=c("black", "red", "blue"), lty=1, lwd=2, legend=c("Sub_metering_1", "Sub_metering_2", "Sub_metering_3"))
## Save to file
dev.copy(png, file="plot3.png", height=480, width=480)
dev.off()
## Download raw zipped data
if(!file.exists("./data")){dir.create("./data")}
fileUrl <- "https://d396qusza40orc.cloudfront.net/exdata%2Fdata%2Fhousehold_power_consumption.zip"
destfile <- "household_power_consumption.zip"
download.file(fileUrl, destfile=paste("data", destfile, sep="/"))
## Unzip the dataset
unzip(paste("data", destfile, sep="/"), exdir="data")
data_dir <- setdiff(dir("data"), destfile)
## Read and subset the txt dataset
dat <- read.csv("./data/household_power_consumption.txt", header=T, sep=';', na.strings="?", nrows=2075259)
dat$Date <- as.Date(dat$Date, format="%d/%m/%Y")
dat <- subset(dat, subset=(Date >= "2007-02-01" & Date <= "2007-02-02"))
## Convert date/time to datetime class
datetime <- paste(dat$Date,dat$Time)
dat <- data.frame(Datetime=as.POSIXct(datetime), dat[-c(1:2)])
class(dat$Datetime)
file.remove(paste0(getwd(),'/data/',dir('data')))
rm(data_dir, destfile, fileUrl, datetime)
## Plot 4
par(mfrow=c(2,2), mar=c(4,4,2,1), oma=c(0,0,2,0))
with(dat, {
plot(Global_active_power~Datetime, type="l",
ylab="Global Active Power (kilowatts)", xlab="")
plot(Voltage~Datetime, type="l",
ylab="Voltage (volt)", xlab="")
plot(Sub_metering_1~Datetime, type="l",
ylab="Global Active Power (kilowatts)", xlab="")
lines(Sub_metering_2~Datetime,col='Red')
lines(Sub_metering_3~Datetime,col='Blue')
legend("topright", col=c("black", "red", "blue"), lty=1, lwd=2, bty="n",
legend=c("Sub_metering_1", "Sub_metering_2", "Sub_metering_3"))
plot(Global_reactive_power~Datetime, type="l",
ylab="Global Rective Power (kilowatts)",xlab="")
})
## Save to file
dev.copy(png, file="plot4.png", height=480, width=480)
dev.off()
## Download raw zipped data
if(!file.exists("./data")){dir.create("./data")}
fileUrl <- "https://d396qusza40orc.cloudfront.net/repdata%2Fdata%2Factivity.zip"
destfile <- "repdata_data_activity.zip"
download.file(fileUrl, destfile=paste("data", destfile, sep="/"))
## Unzip the dataset
unzip(paste("data", destfile, sep="/"), exdir="data")
data_dir <- setdiff(dir("data"), destfile)
dat <- read.csv("./data/repdata_data_activity.txt", header=T, sep=';', na.strings="?")
dat <- read.csv("./data/activity.txt", header=T, sep=';', na.strings="?")
dat <- read.csv("./data/activity.csv", header=T, sep=';', na.strings="?")
dat
dat <- read.csv("./data/activity.csv", header=T, sep=',', na.strings="?")
dat
dat[1:3,]
dat$date <- as.Date(dat$date, format="%d/%m/%Y")
dat <- subset(dat, subset=(date >= "2007-02-01" & date <= "2007-02-02"))
dat
dat <- read.csv("./data/activity.csv", header=T, sep=',', na.strings="?")
dat$date <- as.Date(dat$date, format="%d/%m/%Y")
dat[1:3,]
dat <- read.csv("./data/activity.csv", header=T, sep=',', na.strings="?")
dat[1:3,]
class(dat$date)
as.Date(dat$date)
dat <- read.csv("./data/activity.csv", header=T, sep=',', na.strings="?")
dat$date <- as.Date(dat$date)
dat[1:3,]
dat[1:30,]
dat[1:80,]
dat <- read.csv("./data/activity.csv", header=T, sep=',', na.strings="?")
dat$date <- as.Date(dat$date)
dat <- read.csv("./data/activity.csv", header=T, sep=',')
dat$date <- as.Date(dat$date)
dat[1:30,]
dat
dat <- read.csv("./data/activity.csv", header=T, sep=',', na.strings="?")
dat$date <- as.Date(dat$date)
dat
## Read and subset the txt dataset
dat <- read.csv("./data/activity.csv", header=T, sep=',', na.strings="?", nrows=17568)
dat$date <- as.Date(dat$date)
dat[1:50,]
dat[1:500,]
rm(list=ls())
dat <- read.csv("./data/activity.csv", header=T, sep=',', na.strings="?", nrows=17568)
dat$date <- as.Date(dat$date)
dat
steps.date <- aggregate(steps ~ date, data = activity, FUN = sum)
barplot(steps.date$steps, names.arg = steps.date$date, xlab = "date", ylab = "steps")
steps.date <- aggregate(steps ~ date, data = dat, FUN = sum)
barplot(steps.date$steps, names.arg = steps.date$date, xlab = "date", ylab = "steps")
dat <- read.csv("./data/activity.csv", header=T, sep=',', na.strings="?", nrows=17568)
steps.date <- aggregate(steps ~ date, data = dat, FUN = sum)
barplot(steps.date$steps, names.arg = steps.date$date, xlab = "date", ylab = "steps")
args('aggregate')
class(dat$step)
dat
dat[1:3,]
class(dat$interval)
class(dat$steps)
dat <- read.csv("./data/activity.csv", header=T, sep=',', na.strings="?", nrows=17568)
dat$steps <- as.Date(dat$steps)
dat$date <- as.Date(dat$date)
dat$interval <- as.numeric(dat$interval)
steps.date <- aggregate(steps ~ date, data = dat, FUN = sum)
barplot(steps.date$steps, names.arg = steps.date$date, xlab = "date", ylab = "steps")
## Read and subset the txt dataset
dat <- read.csv("./data/activity.csv", header=T, sep=',', na.strings="?", nrows=17568)
dat$steps <- as.numeric(dat$steps)
dat$date <- as.Date(dat$date)
dat$interval <- as.numeric(dat$interval)
steps.date <- aggregate(steps ~ date, data = dat, FUN = sum)
barplot(steps.date$steps, names.arg = steps.date$date, xlab = "date", ylab = "steps")
# Summarize the data by day
daily_activity <-
aggregate(formula = steps~date, data = dat,
FUN = sum, na.rm=TRUE)
# Calculate summary statistics
mean_steps <- round(mean(daily_activity$steps), 2)  # Mean
median_steps <- quantile(x = daily_activity$steps, probs = 0.5)  # Median, 50%Q
dat
daily_activity
rm(list=ls())
## Download raw zipped data
if(!file.exists("./data")){dir.create("./data")}
fileUrl <- "https://d396qusza40orc.cloudfront.net/repdata%2Fdata%2Factivity.zip"
destfile <- "repdata_data_activity.zip"
download.file(fileUrl, destfile=paste("data", destfile, sep="/"))
## Unzip the dataset
unzip(paste("data", destfile, sep="/"), exdir="data")
data_dir <- setdiff(dir("data"), destfile)
## Read and subset the txt dataset
dat <- read.csv("./data/activity.csv", header=T, sep=',', na.strings="?", nrows=17568)
dat$steps <- as.numeric(dat$steps)
dat$date <- as.Date(dat$date)
dat$interval <- as.numeric(dat$interval)
steps.date <- aggregate(steps ~ date, data = dat, FUN = sum)
barplot(steps.date$steps, names.arg = steps.date$date, xlab = "date", ylab = "steps")
# Summarize the data by day
daily_activity <-
aggregate(formula = steps~date, data = dat,
FUN = sum, na.rm=TRUE)
# Calculate summary statistics
mean_steps <- round(mean(daily_activity$steps), 2)  # Mean
median_steps <- quantile(x = daily_activity$steps, probs = 0.5)  # Median, 50%Q
median_steps
mean_steps
measn_steps
mean_steps
steps.date
dat
steps.date
nrow(dat)
dat[1:30,]
dat
require('knit')
---
title: "PA1_template"
author: "englianhu"
date: "Saturday, August 16, 2014"
output: html_document
---
This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.
When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:
```{r}
summary(cars)
```
You can also embed plots, for example:
```{r, echo=FALSE}
plot(cars)
```
Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
Reproducible Research: Peer Assessment 1
========================================
github repo with RMarkdown source code:
https://github.com/englianhu/RepData_PeerAssessment1
## Loading and preprocessing the data
```{r}
>>>>>>> origin/master
require('ggplot2')
if(!file.exists('./data')){dir.create('./data')}
fileUrl <- 'https://d396qusza40orc.cloudfront.net/repdata%2Fdata%2Factivity.zip'
destfile <- 'repdata_data_activity.zip'
download.file(fileUrl, destfile=paste('data', destfile, sep='/'))
## Unzip the dataset
unzip(paste('data', destfile, sep='/'), exdir='data')
data_dir <- setdiff(dir('data'), destfile)
## Read the dataset
activity <- read.csv('./data/activity.csv', header=T, sep=',', na.strings='?', nrows=17568)
activity$steps <- as.numeric(activity$steps)
activity$date <- as.Date(activity$date)
activity$interval <- as.numeric(activity$interval)
```
## What is mean total number of steps taken per day?
1. Make a histogram of the total number of steps taken each day
```{r}
steps.date <- aggregate(steps ~ date, data=activity, FUN=sum, na.rm=TRUE)
barplot(steps.date$steps, names.arg=steps.date$date, xlab="date", ylab="steps")
```
2. Calculate and report the **mean** and **median** total number of　steps taken per day
```{r}
mean(steps.date$steps)
median(steps.date$steps)
```
## What is the average daily activity pattern?
1. Make a time series plot (i.e. `type = "l"`) of the 5-minute
interval (x-axis) and the average number of steps taken, averaged
across all days (y-axis)
```{r}
steps.interval <- aggregate(steps ~ interval, data=activity, FUN=mean)
plot(steps.interval, type="l")
```
2. Which 5-minute interval, on average across all the days in the
dataset, contains the maximum number of steps?
```{r}
steps.interval$interval[which.max(steps.interval$steps)]
```
## Inputing missing values
1. Calculate and report the total number of missing values in the
dataset (i.e. the total number of rows with `NA`s)
```{r}
sum(is.na(activity))
```
2. Devise a strategy for filling in all of the missing values in the
dataset. The strategy does not need to be sophisticated. For
example, you could use the mean/median for that day, or the mean
for that 5-minute interval, etc.
I will use the means for the 5-minute intervals as fillers for missing
values.
3. Create a new dataset that is equal to the original dataset but with
the missing data filled in.
```{r}
activity <- merge(activity, steps.interval, by="interval", suffixes=c("",".y"))
nas <- is.na(activity$steps)
activity$steps[nas] <- activity$steps.y[nas]
activity <- activity[,c(1:3)]
```
4. Make a histogram of the total number of steps taken each day and
Calculate and report the **mean** and **median** total number of
steps taken per day. Do these values differ from the estimates from
the first part of the assignment? What is the impact of imputing
missing data on the estimates of the total daily number of steps?
```{r}
steps.date <- aggregate(steps ~ date, data=activity, FUN=sum)
barplot(steps.date$steps, names.arg=steps.date$date, xlab="date", ylab="steps")
mean(steps.date$steps)
median(steps.date$steps)
```
The impact of the missing data seems rather low, at least when
estimating the total number of steps per day.
## Are there differences in activity patterns between weekdays and weekends?
1. Create a new factor variable in the dataset with two levels --
"weekday" and "weekend" indicating whether a given date is a
weekday or weekend day.
```{r, cache=TRUE}
daytype <- function(date) {
if (weekdays(as.Date(date)) %in% c("Saturday", "Sunday")) {
"weekend"
} else {"weekday"}}
activity$daytype <- as.factor(sapply(activity$date, daytype))
```
2. Make a panel plot containing a time series plot (i.e. `type = "l"`)
of the 5-minute interval (x-axis) and the average number of steps
taken, averaged across all weekday days or weekend days
(y-axis).
```{r}
par(mfrow=c(2,1))
for (type in c("weekend", "weekday")) {
steps.type <- aggregate(steps ~ interval,
data=activity,
subset=activity$daytype==type,
FUN=mean)
plot(steps.type, type="l", main=type)
}
```
require('knitr')
require('Rmarkdown')
install.packages('slidify')
require('knitr')
activity <- read.csv('./data/activity.csv', header=T, sep=',', na.strings='?', nrows=17568)
rm(list=ls())
library(qsl)
library(sql)
library(sqldf)
## Download raw zipped data
if(!file.exists("./data")){dir.create("./data")}
fileUrl <- "https://d396qusza40orc.cloudfront.net/exdata%2Fdata%2FNEI_data.zip"
destfile <- "exdata_data_NEI_data.zip"
download.file(fileUrl, destfile=paste("data", destfile, sep="/"))
## Unzip the dataset
unzip(paste("data", destfile, sep="/"), exdir="data")
data_dir <- setdiff(dir("data"), destfile)
## This first line will likely take a few seconds. Be patient!
NEI <- readRDS("summarySCC_PM25.rds")
SCC <- readRDS("Source_Classification_Code.rds")
NEI <- readRDS("./data/summarySCC_PM25.rds")
SCC <- readRDS("./data/Source_Classification_Code.rds")
NEI
NEI[1:3,]
SCC[1:3,]
Emissions <- aggregate(NEI[, 'Emissions'], by=list(NEI$year), FUN=sum)
Emissions
round(Emissions[,2]/1000,2)
NEI[1:3,]
plot(Emissions$x~Emissions$Group.1)
plot(Emissions$x~Emissions$Group.1,type='l')
hist(Emissions$x)
barplot(Emissions$PM, names.arg=Emissions$Group.1,
main=expression('Total Emission of PM'[2.5]),
xlab='Year', ylab=expression(paste('PM', ''[2.5], ' in Kilotons')))
Emissions$PM <- round(Emissions[,2]/1000,2)
Emissions
barplot(Emissions$PM, names.arg=Emissions$Group.1,
main=expression('Total Emission of PM'[2.5]),
xlab='Year', ylab=expression(paste('PM', ''[2.5], ' in Kilotons')))
aggregate(Emissions~year, data=NEI, FUN=sum)
aggregate(NEI[, 'Emissions'], by=list(NEI$year), FUN=sum)
Emissions
SCC[1:4,]
png(filename='plot1.png')
dev.off()
png(filename='plot1.png')
barplot(Emissions$PM, names.arg=Emissions$Group.1,
main=expression('Total Emission of PM'[2.5]),
xlab='Year', ylab=expression(paste('PM', ''[2.5], ' in Kilotons')))
dev.off()
rm(list=ls())
## Read the dataset
NEI <- readRDS("./data/summarySCC_PM25.rds")
SCC <- readRDS("./data/Source_Classification_Code.rds")
# Aggregate
Emissions <- aggregate(Emissions~year, data=NEI, FUN=sum)
Emissions$PM <- round(Emissions[,2]/1000,2)
# Subset data and append two years in one data frame
MD <- subset(NEI, fips=='24510')
# Have total emissions from PM2.5 decreased in the Baltimore City, Maryland (fips == "24510")
# from 1999 to 2008? Use the base plotting system to make a plot answering this question.
# Generate the graph in the same directory as the source code
png(filename='plot2.png')
barplot(Emissions$PM, names.arg=Emissions$Group.1,
main=expression('Total Emission of PM'[2.5]),
xlab='Year', ylab=expression(paste('PM', ''[2.5], ' in Kilotons')))
dev.off()
## Download raw zipped data
if(!file.exists("./data")){dir.create("./data")}
fileUrl <- "https://d396qusza40orc.cloudfront.net/exdata%2Fdata%2FNEI_data.zip"
destfile <- "exdata_data_NEI_data.zip"
download.file(fileUrl, destfile=paste("data", destfile, sep="/"))
## Unzip the dataset
unzip(paste("data", destfile, sep="/"), exdir="data")
data_dir <- setdiff(dir("data"), destfile)
## Read the dataset
NEI <- readRDS("./data/summarySCC_PM25.rds")
SCC <- readRDS("./data/Source_Classification_Code.rds")
# Aggregate
Emissions <- aggregate(Emissions~year, data=NEI, FUN=sum)
Emissions$PM <- round(Emissions[,2]/1000,2)
# Subset data and append two years in one data frame
MD <- subset(NEI, fips=='24510')
# Have total emissions from PM2.5 decreased in the Baltimore City, Maryland (fips == "24510")
# from 1999 to 2008? Use the base plotting system to make a plot answering this question.
# Generate the graph in the same directory as the source code
png(filename='plot2.png')
barplot(Emissions$PM, names.arg=Emissions$Group.1,
main=expression('Total Emission of PM'[2.5]),
xlab='Year', ylab=expression(paste('PM', ''[2.5], ' in Kilotons')))
dev.off()
## Read the dataset
NEI <- readRDS("./data/summarySCC_PM25.rds")
SCC <- readRDS("./data/Source_Classification_Code.rds")
# Aggregate
Emissions <- aggregate(Emissions~year, data=NEI, FUN=sum)
Emissions$PM <- round(Emissions[,2]/1000,2)
# Subset data and append two years in one data frame
MD <- subset(NEI, fips=='24510')
# Have total emissions from PM2.5 decreased in the Baltimore City, Maryland (fips == "24510")
# from 1999 to 2008? Use the base plotting system to make a plot answering this question.
# Generate the graph in the same directory as the source code
png(filename='plot2.png')
barplot(tapply(X=MD$Emissions, INDEX=MD$year, FUN=sum),
main='Total Emission in Baltimore City, MD',
xlab='Year', ylab=expression('PM'[2.5]))
dev.off()
require(ggplot2)
# Baltimore City, Maryland == fips
MD <- subset(NEI, fips == 24510)
MD$year <- factor(MD$year, levels=c('1999', '2002', '2005', '2008'))
# Of the four types of sources indicated by the type (point, nonpoint, onroad, nonroad) variable,
# which of these four sources have seen decreases in emissions from 1999â2008 for Baltimore City?
# Which have seen increases in emissions from 1999â2008?
# Use the ggplot2 plotting system to make a plot answer this question.
# Generate the graph in the same directory as the source code
png(filename='plot3.png', width=800, height=500, units='px')
ggplot(data=MD, aes(x=year, y=log(Emissions))) + facet_grid(. ~ type) + guides(fill=F) +
geom_boxplot(aes(fill=type)) + stat_boxplot(geom ='errorbar') +
ylab(expression(paste('Log', ' of PM'[2.5], ' Emissions'))) + xlab('Year') +
ggtitle('Emissions per Type in Baltimore City, Maryland') +
geom_jitter(alpha=0.10)
dev.off()
## Read the dataset
NEI <- readRDS("./data/summarySCC_PM25.rds")
SCC <- readRDS("./data/Source_Classification_Code.rds")
# Coal combustion related sources
SCC.coal = SCC[grepl("coal", SCC$Short.Name, ignore.case=TRUE),]
# Merge two data sets
merge <- merge(x=NEI, y=SCC.coal, by='SCC')
merge.sum <- aggregate(merge[, 'Emissions'], by=list(merge$year), sum)
colnames(merge.sum) <- c('Year', 'Emissions')
# Across the United States, how have emissions from coal combustion-related sources
# changed from 1999-2008?
# Generate the graph in the same directory as the source code
png(filename='plot4.png')
ggplot(data=merge.sum, aes(x=Year, y=Emissions/1000)) +
geom_line(aes(group=1, col=Emissions)) + geom_point(aes(size=2, col=Emissions)) +
ggtitle(expression('Total Emissions of PM'[2.5])) +
ylab(expression(paste('PM', ''[2.5], ' in kilotons'))) +
geom_text(aes(label=round(Emissions/1000,digits=2), size=2, hjust=1.5, vjust=1.5)) +
theme(legend.position='none') + scale_colour_gradient(low='black', high='red')
dev.off()
NEI$year <- factor(NEI$year, levels=c('1999', '2002', '2005', '2008'))
# Baltimore City, Maryland == fips
MD.onroad <- subset(NEI, fips == 24510 & type == 'ON-ROAD')
# Aggregate
MD.df <- aggregate(MD.onroad[, 'Emissions'], by=list(MD.onroad$year), sum)
colnames(MD.df) <- c('year', 'Emissions')
# How have emissions from motor vehicle sources changed from 1999-2008 in Baltimore City?
# Generate the graph in the same directory as the source code
png('plot5.png')
ggplot(data=MD.df, aes(x=year, y=Emissions)) + geom_bar(aes(fill=year)) + guides(fill=F) +
ggtitle('Total Emissions of Motor Vehicle Sources in Baltimore City, Maryland') +
ylab(expression('PM'[2.5])) + xlab('Year') + theme(legend.position='none') +
geom_text(aes(label=round(Emissions,0), size=1, hjust=0.5, vjust=2))
dev.off()
png('plot5.png')
ggplot(data=MD.df, aes(x=year, y=Emissions)) + geom_bar(aes(fill=year)) + guides(fill=F) +
ggtitle('Total Emissions of Motor Vehicle Sources in Baltimore City, Maryland') +
ylab(expression('PM'[2.5])) + xlab('Year') + theme(legend.position='none')
dev.off()
require('swirl')
install.packages('swirl')
require('swirl')
install_from_swirl('Regression Models')
swirl()
swirl()
q('no')
swirl()
require(swirl)
swirl()
install_from_swirl('Regression Models')
install.packages('UsingR')
install.packages(c("bdsmatrix", "DBI", "dendextend", "formatR", "gsubfn", "htmltools", "httr", "jsonlite", "labeling", "maps", "markdown", "mime", "Rcpp", "spam", "testthat", "xlsx", "xlsxjars"))
Skip to content
This repository
Explore
Gist
Blog
Help
Eng Lian Hu englianhu
1
0
0
englianhu/Statistical_Inference
Statistical_Inference / StatsInf_Assignment.R
Eng Lian Hu englianhu 29 days ago
Update
1 contributor
20 lines (19 sloc) 0.648 kb
mydir <- paste0(getwd(),'/GitHub/englianhu/Statistical_Inference')
setwd(mydir)
library(datasets)
<<<<<<< HEAD
=======
library(knitr)
library(markdown)
library(formatR)
library(Hmisc)
library(reshape)
library(ggplot2)
library(car)
# knitr configuration
opts_knit$set(progress=FALSE)
opts_chunk$set(echo=TRUE, message=FALSE, tidy=TRUE, comment=NA,
fig.path="figure/", fig.keep="high", fig.width=10, fig.height=6,
fig.align="center")
knit2html('StatsInf_Assignment.Rmd')
browseURL('StatsInf_Assignment.html')
# rpubsUpload(title, htmlFile, id = NULL, properties = list(),
# method = getOption("rpubs.upload.method", "internal"))
mydir <- paste0(getwd(),'/GitHub/englianhu/Statistical_Inference')
setwd(mydir)
library(datasets)
>>>>>>> origin/master
library(knitr)
library(markdown)
library(formatR)
library(Hmisc)
library(reshape)
library(ggplot2)
library(car)
# knitr configuration
opts_knit$set(progress=FALSE)
opts_chunk$set(echo=TRUE, message=FALSE, tidy=TRUE, comment=NA,
fig.path="figure/", fig.keep="high", fig.width=10, fig.height=6,
fig.align="center")
knit2html('StatsInf_Assignment.Rmd')
browseURL('StatsInf_Assignment.html')
# rpubsUpload(title, htmlFile, id = NULL, properties = list(),
#             method = getOption("rpubs.upload.method", "internal"))
install.packages(c('reshape','survival'))
knit2pdf('StatsInf_Assignment.Rmd')
rm(list=ls())
